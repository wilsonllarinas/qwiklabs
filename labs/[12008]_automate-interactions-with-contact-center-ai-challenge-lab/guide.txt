# Automate Interactions with Contact Center AI: Challenge Lab
# https://www.qwiklabs.com/focuses/12008?parent=catalog

# Setup
git clone https://github.com/GoogleCloudPlatform/dataflow-contact-center-speech-analysis.git

# Task 1: Create a Regional Cloud Storage bucket
- Go to Storage > Bucket > Create bucket
    - Use your project id as name
    - LOCATION MUST BE IN us-central1

# Task 2: Create a Cloud Function
- Go to Cloud Function > Create Function
    - name: use default
    - Trigger: Cloud Storage
        - Event Type: Finalize/Create
        - Bucket: <your_bucket_name>
    - Runtime: Node.js 8
    - index.js: use this code https://github.com/GoogleCloudPlatform/dataflow-contact-center-speech-analysis/blob/master/saf-longrun-job-func/index.js
    - package.json: use this code https://github.com/GoogleCloudPlatform/dataflow-contact-center-speech-analysis/blob/master/saf-longrun-job-func/package.json
    - Entry point: safLongRunJobFunc
    - Click Environment Variables, Networking, Timeouts and more. Make sure region is us-central1
    - DEPLOY

# Task 3: Create a BigQuery dataset
- Go to BigQuery > Create dataset with Name: lab

# Task 4: Create a Pub/Sub topic
- Go to Pub/Sub > Topics > Create Topic with Topic ID: speech2text

# Task 5: Create a Regional Cloud Storage bucket with DFaudio folder
- Back to your cloud storage, click your bucket and create a folder called DFaudio

# Task 6: Deploy Dataflow pipeline
- Open cloud shell, run:
# make sure you already clone the repo in # Setup
cd dataflow-contact-center-speech-analysis/saf-longrun-job-dataflow
python -m virtualenv env -p python3
source env/bin/activate
pip install apache-beam[gcp]
pip install dateparser
export PROJECT_ID=[YOUR_PROJECT_ID]
export TOPIC_NAME=speech2text
export BUCKET_NAME=[YOUR_BUCKET_NAME]
export DATASET_NAME=lab
export TABLE_NAME=transcript
python3 saflongrunjobdataflow.py --project=$PROJECT_ID --input_topic=projects/$PROJECT_ID/topics/$TOPIC_NAME --runner=DataflowRunner --region=us-central1 --temp_location=gs://$BUCKET_NAME/tmp --output_bigquery=$DATASET_NAME.$TABLE_NAME --requirements_file="requirements.txt"
- Check your pipeline in Dataflow

# Task 7: Process the sample audio files
- Open cloud shell, run:
gsutil -h x-goog-meta-callid:1234567 -h x-goog-meta-stereo:false -h x-goog-meta-pubsubtopicname:$TOPIC_NAME -h x-goog-meta-year:2019 -h x-goog-meta-month:11 -h x-goog-meta-day:06 -h x-goog-meta-starttime:1116 cp gs://qwiklabs-bucket-gsp311/speech_commercial_mono.flac gs://$BUCKET_NAME/DFaudio
gsutil -h x-goog-meta-callid:1234567 -h x-goog-meta-stereo:true -h x-goog-meta-pubsubtopicname:$TOPIC_NAME -h x-goog-meta-year:2019 -h x-goog-meta-month:11 -h x-goog-meta-day:06 -h x-goog-meta-starttime:1116 cp gs://qwiklabs-bucket-gsp311/speech_commercial_stereo.wav gs://$BUCKET_NAME/DFaudio

# Task 8: Run a Data Loss Prevention Job 
- Back to BigQuery
- Click new generated table (transcript table)
- Click More > Query Settings
    - Destination: Choose Set a destination table for query result
    - Table name: copied
- SELECT * FROM `[YOUR_PROJECT_ID].lab.transcript`
    # expected row of output: 1 row
- Click at generated copied table
- Click EXPORT > Scan with DLP
    - Job ID: scan-copied
    - leave all default > CREATE
    - Wait its status to Done, check your progress

